---
title: "Quantitative Disk Assay"
author: "Aleeza C. Gerstein"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Quantitative Disk Assay}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc} 
---

```{r setup, message=FALSE, echo=FALSE}
library(knitr)
# This is necessary to direct knitr to find the 
# 'data', and other directories that contain
# files needed to execute this document
# thanks to http://stackoverflow.com/a/24585750/1036500
# opts_knit$set(root.dir=normalizePath('../'))
# opts_chunk$set(fig.path = "../figures/")
```

```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

```{r, echo=FALSE}
library("diskImageR")
```


## Introduction to diskImageR
diskImageR provides a quantitative, unbiased method to analyze photographs of disk diffusion assays for any microbial species/microbial drug combination. This computational method measures the radius of inhibition ("RAD", i.e., resistance) at three different cutoff values (80%, 50% and 20% growth inhibition) as well as two measures of tolerance, the fraction of growth achived above RAD ("FoG"), and drug sensitivity ("slope", the rate of change from no growth to full growth. 

<center>
<img src="pictures/p2_30_a.jpg"  style="width: 35%; height: 35%" style="float:left," alt="" /> <img src="pictures/p1_30_a.JPG"  style="width: 35%; height: 35%" style="float:left," alt="" /> 
</center> 

## Prepare plates and photographs
The analysis done by diskImageR will only be as good as the photographs taken of the disk assay plates. We use the Bencher Copymate II camera mounting system, though any setup that holds the camera a fixed distance from the disk assay plates should work. In this setup there are two fluorescent lights on either side of the disk, oriented to minimize shadows on the plate in an otherwise dark room. We use the Canon Rebel T3i camera with an ISO 800, white blaance "white fluorescent light", time 1/100s, picture stype "neutral", centre focused. Any camera of reasonably high quality should suffice, though the camera should always be set in manual rather than automatic mode, as the goal is to take photographs as consistently as possible. We also use the 2s timer to avoid potentially jostling the camera while taking images and/or having a hand/arm shadow in the picture. Plates should be photographed on a dark surface (we use black velvet) and plate labels are written on the side rather than the bottom of the plate. Prior to analysis, images are cropped close to the plate. Because the analysis program automatically detects the disk based on size, it is important that no other similar-sized circles be present in the image (e.g., from letters in labels).

Once you have the set of photographs that you want to be analyzed together they should be placed in the same directory, with nothing else inside that directory. 
<b>Important!</b> If there are any other files within the directory the script will not run properly.

The photograph file naming scheme will be be carried throughout, thus care should be taken with naming files in a logical manner. The general format that is compatible with the script default setting is "strain_factor1_factor2_rep.pdf". Conversely, if you intend to do statistical analysis separately (i.e., averaging among replicates), photographs can be named anything.

## Run the imageJ macro on the set of photographs
The first step in the diskImageR pipeline is to analyze each photograph in imageJ.

<b> Important! </b> imageJ must be installed on your computer. ImageJ is a free, public domain Java image proessing program available for download <a href="http://rsb.info.nih.gov/ij/download.html"> here</a>. Take note of the path to imageJ, as this will be needed for the first function. If you install it to default location (Applications folder on a mac, Program Files folder on a PC) then you should be able to use the default setting (see below). 

From each photograph, the macro (in imageJ) will automatically determine where the disk is located on the plate, find the center of the disk, and draw 40mm radial lines out from the center of the disk every 5 degrees. For each line, the pixel intensity will be determined at many points along the line. This data will be stored in the folder *imageJ-out* on your computer, with one file for each photograph.

The first command of the package is \link{code{IJMacro}}, which can be run in two different ways, either through a user-interface with pop-up boxes, or directly through the shell. At this point you will specify a project name, the main project directory, and the photograph directory.
The project name should ideally be fairly short (easy to type without typos!) and specific to the project. It must start with a letter, not a number or special character, but can otherwise be anything.
The mmain project directory is the place where all folders and files and generated by the package will show up.
The photograph directory is the one used to store photographs from above (which has nothing except the photographs to be analyzed in it).

<b> Important! </b> There can not be any spaces or special characters in any of the folder names that are in the path that lead to either the main project directory or the photograph directory.

To run the imageJ macro through a user-interface with pop-up boxes: 
```r
IJMacro("newProject")
```

If you would prefer to avoid pop-up boxes you can specify your desired main project directory and photograph directory locations:
```r
IJMacro("newProject", projectDir= "/path/to/projectDir", photoDir = "/path/to/projectDir/phogoraphs/")
```

```{r, echo=FALSE}
IJMacro("newProject", projectDir= getwd(), photoDir = file.path(getwd(), "pictures", ""))
```

If the function is unable to locate imageJ a red error will pop-up with a message like <span style="color:red">/bin/sh: /Applications/ImageJ/ImageJ.app/Contents/MacOS/JavaApplicationStub: No such file or directory</span>. The easiest solution is to move imageJ to the default location or to specify the path to imageJ with argument imageJLoc = "/path/to/imageJ".

<b> Important! </b> This step must run completely, without error, for everything downstream. 

To access the output of \code{IJMacro} in a later R session use \link{code{readExistingIJ}}. At this step you can also change the project name, it does not have to be the same name that was used originally.
```r
readExistingIJ("newProject") 
```

## Plot the output of imageJ analysis
\code{plotRaw} will create a pdf figure of plots showing the average pixel intensity across all 72 lines from each photograph. It is not required for anything downstream, but is a good check to see whether the analysis proceeded properly. 

```{r, fig.width=6, fig.height=4}
plotRaw("newProject", popUp = FALSE, savePDF = FALSE)
```

Many different factors can be specified to influence the plots and the pdf that is generated, including the minimum and maximum x and y values (xmin, xmax, ymin, ymax), the number of plots in each row (xplots), the height and width of the pdf file (height, width), the point size (cexPt), and the size of the x- and y-axis font (cexX, cexY). See ?plotRaw for all options and to see default values.

## Run the maximum likelihood analysis 
The next step is to use maximum likelihood to find the logistic and double logistic equations that best describe the shape of the imageJ output data. These data follow a characteristic "S-shape" curve, so the standard logistic equation is used where asym is the asymptote, od50 is the midpoint, and scal is the slope at od50 divided by asym/4.
$$
y = \frac{asym*exp(scal(x-od50))}{1+exp(scal(x-od50))}+N(0, \sigma)
$$

We often observed disk assays that deviated from the single logistic, either rising more linearly than expected at low cell density, or with an intermediate asymptote around the midpoint. To fascilitate fitting these curves, we fit a double logistic, which allows greater flexibility. Our primary goal in curve fitting is to capture an underlying equation that fits the observed data, rather than to test what model fits better.
$$
y = \frac{asymA*exp(scalA(x-od50A))}{1+exp(scalA(x-od50A))}+\frac{asymB*exp(scalB(x-od50B))}{1+exp(scalB(x-od50B))}+N(0, \sigma)
$$

From these functions we substract off the plate background intensity from all values; this is common across all pictures taken at the same time and is determined from the observed pixel intensity on a plate with a clear halo (specified by the user). We then use the parameters identified in the logistic equations to determine the resistance parameters.

* <b>Resistance</b>
	: asymA+asymB are added together to determine the maximum level of intensity (= cell density) achieved on each plate. The level of resistance (zone of inhibition, RAD), is calculated by asking what x value (distance in mm) corresponds to the point where 80%, 50% and 20% reduction in growth occurs (corresponding to *RAD80*, *RAD50*, and *RAD20*)
* <b>perseverence</b>
	: the 'rollmean' function from the zoo package is used to calculate the area under the curve (FoG)  in slice from the disk edge to each RAD cutoff. This achieved growth is then compared to the potential growth, namely, the area of a rectangle with length and height equal to the RAD. The calculated paramaters are thus the fraction of full growth in this region (*FoG80*, *fACU50*, *FoG20*).
* <b>Sensitivity</b>
	: the ten data points on either side of the midpoint (od50) from the single logistic equation are used to find the slope of the best fit linear model using the lm function in R.

```{r,  fig.width=5, fig.height=4}
maxLik("newProject", clearHalo=1, RAD="all", needML=TRUE, popUp = FALSE, savePDF = FALSE, FoG=20)
```

### [OPTIONAL] Save the maximum likelihood results
It is possible to save the maximum likelihood results using
```
saveMLParam("newProject")
```
This will save a .csv file into the *paramter_files* directory that contains parameter estimates for asym, od50, scal and sigma, as well as the log likelihood of the single and double logistic models.
 
## Save the results 
The last required step creates and save a dataframe with the resistance parameter estimates. A .csv file is written to the *parameter_files* directory which can be opened in excel or any program that opens text files. 

```{r}
createDataframe("newProject", clearHalo = 1, typeName="temp")
newProject.df
```

### [OPTIONAL] Add additional factor columns
If your photograph names contain more than one factor that is important (i.e, if your files names look like: line_factor1_factor2...") you can add extra factors into the dataframe using

```{r}
addType("newProject", typeName="rep")
newProject.df
```
If you want to access this dataframe in a later R session you can do so using readExistingDF("betterName"). Any project name can be used here, not only the previous name. This file can also be loaded in standard ways (e.g., temp <- read.csv(file)) though if you intend to use the functions below, you need to save it with a name that ends with ".df" (i.e., temp.df).

### [OPTIONAL] Aggregate replicate pictures
This function is useful if you have many replicate disk assays and want to calculate their average and variance. The function will calculate the standard error (se), coefficient of variantion (CV) or generic R variance measures (e.g., standard deviation, sd). 

For this example I am loading an existing dataset tha I will call "manyReps". This dataset contains data for seven different lines, with twelve replicates per line, and a factor I'm interested in that has two two levels. I can then use the function aggregateData to average among the 12 replicates and calculate their standard deviation. Note that you can "cheat" - as long as you save your dataframe with .df at the end of the name you can run \code{\link{aggregateData}} on any dataframe, not just one imported with \link\code{readExistingDF}}.

```{r}
manyReps.df <- read.csv(file.path(getwd(), "data", "manyReps_df.csv"))
head(manyReps.df)
```

```{r}
aggregateData("manyReps", replicate=c("line", "type"), varFunc="se")
manyReps.ag
```
This will also save a .csv file into the *parameter_files* directory.

### [OPTIONAL] Plot parameter results
At this point the parameter esitmates and potentially aggregated and variance measurements exist in .csv files and can be used independently of the package. Three plotting functions are included here though, to plot single parameters (any) \link{\code{oneParamPlot}}, two parameters (RAD and FoG) \link{\code{twoParamPlot}} or three parameters (RAD, slope, FoG) \link\code{threeParamPlot}}. Input can be the dataframe from either \link{code{createDataframe}} \code{type="df"} or from \link{code{aggregateData}} \code{type=="ag"}. Single parameter plot can be either a barplot \code{barplot = TRUE} or a dotplot \code{barplot=FALSE}. In the two and three parameter plots the default is to plot perseverence as a barplot and RAD and slope as a dotplot, perseverence can also be plotted as a dotplot with \code{barplot=FALSE} though there is currently not support to plot either RAD or slope as a barplot in this framework. 

```{r, fig.width=5, fig.height=4}
twoParamPlot("manyReps", type= "ag", popUp = TRUE, savePDF =FALSE, xlabAngle = -45, order = c(1, 8, 2, 9, 3, 10, 4, 11, 5, 12, 6, 13, 7, 14), xlabels =paste(rep(manyReps.ag$line[1:7], each=2), rep(c("A", "B"), 7), sep="-"))
```




## Addendum
#### Forthcoming
* basic t-tests (t.test)
* basic anova (aov)
* single parameter graphics
* three parameter graphics

#### Acknowledgements
Thank you to Adi Ulman for the original motivation, Noa Blutraich, Gal Benron and Alex Rosenberg for testing versions of the code presented here, and Darren Abbey and particularly Judith Berman for philosophical discussions about how best to computationally capture the biological variation observed in the disk assay.

#### Contact
Aleeza Gerstein, <gerst035@umn.edu>

### Updated
Last (mini-update) July 2015
